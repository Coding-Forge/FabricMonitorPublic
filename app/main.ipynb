{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fabric Monitor  \n",
    "\n",
    "This notebook is the main notebook from which we can call specific functions to run tasks like getting data for activity, catalog, graph, and tenant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "packages to be used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01midentity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ClientSecretCredential\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mazure\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstorage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfiledatalake\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FileSystemClient, DataLakeDirectoryClient\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutility\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bob\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutility\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m File_Table_Management\n",
      "File \u001b[0;32m~/projects/fabric/monitor/FabricMonitor/app/utility/helper.py:8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime, timedelta\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01madal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AuthenticationContext\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutility\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m File_Table_Management\n\u001b[1;32m     10\u001b[0m logging\u001b[38;5;241m.\u001b[39mbasicConfig(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyapp.log\u001b[39m\u001b[38;5;124m'\u001b[39m, level\u001b[38;5;241m=\u001b[39mlogging\u001b[38;5;241m.\u001b[39mINFO)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBob\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from adal import AuthenticationContext\n",
    "from azure.identity import ClientSecretCredential\n",
    "from azure.storage.filedatalake import FileSystemClient, DataLakeDirectoryClient\n",
    "from utility.helper import Bob\n",
    "from utility.fabric import File_Table_Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Functions\n",
    "\n",
    "functions and classes are held in the utility folder. The folder has `helper.py` and `fabric.py`. These files contain `Bob` and `File_Table_Management` respectively\n",
    "\n",
    "> To work with files on lakehouse or warehouse see examples  \n",
    "> using service principals with fabric lakehouse and warehouse  \n",
    "> [Fabric Service Principals](https://debruyn.dev/2023/how-to-use-service-principal-authentication-to-access-microsoft-fabrics-onelake/)  \n",
    " > [Microsoft Docs](https://github.com/MicrosoftDocs/fabric-docs/blob/main/docs/onelake/onelake-access-python.md)  \n",
    " \n",
    " ```\n",
    "     paths = file_system_client.get_paths(path=f\"/{lakehouse name}.Lakehouse/Files/\")\n",
    "     for p in paths:\n",
    "         print(p.name)\n",
    " \n",
    " \n",
    "     paths = file_system_client.get_paths(path=f\"/{warehouse name}.Warehouse/Tables/\")\n",
    "     for p in paths:\n",
    "         print(p.name)\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputBatchCount = 5000\n",
    "api_root = \"https://api.powerbi.com/v1.0/myorg/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Modified Activities since last run date\n",
    " \n",
    "Get the lastRun date that is written in the state file. this date will be used to get workspaces that have been modified after the recorded date\n",
    " \n",
    "```\n",
    "activity = dict()\n",
    "activity[\"lastRun\"]= (datetime.now() + timedelta(days=-15)).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    " \n",
    " \n",
    "with open('settings.json', 'w') as file:\n",
    "     file.write(json.dumps(activity))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'OutputPath': '../Data', 'StorageAccountContainerRootPath': '', 'StorageAccountConnStr': 'DefaultEndpointsProtocol=https;AccountName=powerbimonitor91ed;AccountKey=KIFypgGgseWdcevH6RfBxo6cbmf9h1aGBqJTp1z9Rd7vPHL0DXWojFlIDu6dOlfCbBg53xWL4wgv+AStISnOAg==;EndpointSuffix=core.windows.net', 'StorageAccountContainerName': 'pbimonitor', 'FullScanAfterDays': '30', 'CatalogGetInfoParameters': 'lineage=true&datasourceDetails=true&getArtifactUsers=true&datasetSchema=true&datasetExpressions=true', 'CatalogGetModifiedParameters': 'excludePersonalWorkspaces=false&excludeInActiveWorkspaces=true', 'ServicePrincipal': {'AppId': 'e15fadf9-bc05-4ade-af9f-d79a918bbacd', 'AppSecret': 'gCa8Q~w9zuEnkl9SBCo7OXcwriGBA7C26nGTIdzs', 'TenantId': '0b69ab40-1bc7-4666-9f20-691ba105a907', 'Environment': 'Public'}, 'Subscription_ID': '4465cf7c-8bde-41f8-aa38-938da8ac30a9', 'Subscription_Name': 'ME-MngEnvMCAP084084-brcampb-1', 'Function_App_Plan_Name': 'coding-forge-app-plan', 'Resource_Group_Name': 'Coding-Forge', 'LastRun': '2024-01-08', 'GraphExtractGroups': 'false', 'GraphPaginateCount': 10000, 'LakehouseName': 'FabricLake', 'WarehouseName': 'FabricWarehouse'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../config.json', 'r') as file:\n",
    "    f = file.read()\n",
    "\n",
    "if isinstance(f, str):\n",
    "    app_settings = json.loads(f)\n",
    "    print(app_settings)\n",
    "else:\n",
    "    g = json.dumps(f)\n",
    "\n",
    "    g.get(\"OutputPath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Data'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_settings['OutputPath']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api.powerbi.com/v1.0/myorg/admin/admin/apps?$top=5000&$skip=0\"\n",
    "\n",
    "mkme = [{\"name\":\"brandon\",\"age\":30}, {\"name\":\"bob\",\"age\":40}]\n",
    "\n",
    "\n",
    "\n",
    "dc = FF.create_directory(file_system_client=FF.fsc, directory_name=lakehousePath)\n",
    "\n",
    "# do a for loop until all json arrays in audits are read and written to storage\n",
    "for audit in audits:\n",
    "\n",
    "    if pageIndex == 1:\n",
    "        outputFilePath = f\"{outputPath}/{pivotDate.strftime('%Y%m%d')}.json\"\n",
    "        lakehouseFile = f\"{pivotDate.strftime('%Y%m%d')}.json\"\n",
    "    else:\n",
    "        outputFilePath = f\"{outputPath}/{pivotDate.strftime('%Y%m%d')}_{pageIndex}.json\"\n",
    "        lakehouseFile = f\"{pivotDate.strftime('%Y%m%d')}_{pageIndex}.json\"\n",
    "\n",
    "    # TODO: convert audits to json\n",
    "    with open(outputFilePath, \"w\") as file:\n",
    "        file.write(json.dumps(audit))\n",
    "\n",
    "    FF.upload_file_to_directory(directory_client=dc, local_path=outputPath, file_name=lakehouseFile)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
